description: run on hai6

target:
  service: amlk8s
  # run "amlt target list amlk8s" to list the names of available AMLK8s targets
  # TARGET_NAME       SERVICE    CLUSTER           VC        WORKSPACE_NAME    RESOURCE_GROUP    METADATA
  # ----------------  ---------  ----------------  --------  ----------------  ----------------  --------------------------------------------------
  # itpeastusv100cl2  amlk8s     itpeastusv100cl2  resrchvc  resrchvc-eus      researchvc-eus    card: V100, gpus_per_vm: 4, ib: yes, gpu_mem: 16GB
  # itpeusp100cl      amlk8s     itpeusp100cl      resrchvc  resrchvc-eus      researchvc-eus    card: P100, gpus_per_vm: 4, ib: yes, gpu_mem: 16GB
  # itpwus2cpucl1     amlk8s     itpwus2cpucl1     gcrcpu    gcrcpu            gcrcpu-wus2       card: CPU, gpus_per_vm: 0, ib: no, gpu_mem: 128GB
  # itpseasiav100cl   amlk8s     itpseasiav100cl   resrchvc  resrchvc-sea      researchvc-sea    card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # itplabrr1cl1      amlk8s     itplabrr1cl1      resrchvc  resrchvc          researchvc        card: V100, gpus_per_vm: 8, ib: yes, gpu_mem: 32GB
  # itpwus2v100cl     amlk8s     itpwus2v100cl     resrchvc  resrchvc          researchvc        card: V100, gpus_per_vm: 8, ib: yes, gpu_mem: 32GB
  # itpeusp40cl       amlk8s     itpeusp40cl       resrchvc  resrchvc-eus      researchvc-eus    card: P40, gpus_per_vm: 4, ib: no, gpu_mem: 24GB
  # itpscusv100cl     amlk8s     itpscusv100cl     resrchvc  resrchvc-sc       researchvc-sc     card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # itplabrl1cl1      amlk8s     itplabrl1cl1      resrchvc  resrchvc          researchvc        card: P100, gpus_per_vm: 1, ib: yes, gpu_mem: 16GB
  # itpeastusv100cl   amlk8s     itpeastusv100cl   resrchvc  resrchvc-eus      researchvc-eus    card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # name: itpeastusv100cl2
  # name: itpeusp100cl
  name: itphyperdgx2cl1
  vc: hai6
  # name: ms-shared

environment:
  # image: publicrepository/bert:pytorch_1.4_nccl_2.7.8_horovod_0.19.5_transformers_0.5.0_ib
  # image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel
  # registry: docker.io # any public registry can be specified here
  image: my_azureml-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu:latest
  registry: hai6cr.azurecr.io
  username: hai6cr
  setup:
    - export NCCL_DEBUG=WARN
    - pip install -r requirements.txt
    # - export NCCL_P2P_DISABLE=1

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/

data:
  # You need to run "python src/download_data.py" beforehand
  # to generate the dataset to be uploaded
  # don't forget to run with --upload-data
  local_dir: $CONFIG_DIR/data/

  # The data will be uploaded to your default storage.
  #   Check ``multi_storage.yaml'' for more flexibility.
  remote_dir: data/

# storage:
#   shared_data:
#     storage_account_name: haotian
#     container_name: home
#     mount_dir: /mnt/shared_data

env_defaults:
  BLOCK: 2048

# list of jobs to run
# avalaible sku options: ['G0', 'G1', 'G2', 'G4', 'G8', 'G16',
# '16G1', '16G4', '16G8', '24G1', '24G2', '24G4', '24G8',
# '32G4', '32G8', '32G16', '64G4', '40G8']
jobs:
  - name: stage1_finetune_hai6
    sku: G16
    command:
      - bash finetune-gpt-neo_cli.sh 16
    # submit_args:
    #   container_args:
    #     cpus: 16
    #     memory: 300g
  - name: stage1_finetune_hai6_gpt2_bs$BLOCK
    sku: G16
    command:
      - bash finetune-gpt-neo_cli.sh -j ft-stage1-gpt2 -m gpt2 -s $BLOCK 16
  - name: stage2_finetune_gpt-neo_hai6
    sku: G16
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage2-gpt-neo 
        -m "/mnt/default/ft-stage1-gpt-neo-GithubCodeDoc-Jan16-16-49-2022/checkpoint-80000" 16
  - name: stage2_finetune_gpt2_hai6
    sku: G16
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage2-gpt2
        -m "/mnt/default/ft-stage1-gpt2-GithubCodeDoc-Jan15-12-53-2022/checkpoint-172500" 16
  - name: stage1_finetune_gpt-neo-27_bs$BLOCK
    sku: G16
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage1-gpt-neo-27
        -m "EleutherAI/gpt-neo-2.7B" -s $BLOCK 16
  - name: stage2_finetune_gpt-neo-27_bs$BLOCK
    sku: G16
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage2-gpt-neo-27
        -m "/mnt/default/ft-stage1-gpt-neo-27-GithubCodeDoc-Jan17-05-10-2022/checkpoint-55000" -s $BLOCK 16
  - name: stage1_finetune_gpt-j_bs$BLOCK
    sku: G16
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage1-gpt-j
        -m "EleutherAI/gpt-j-6B" -s $BLOCK 16
  - name: stage1_finetune_gpt-j_2xG16
    sku: 2xG16
    aml_mpirun:
      process_count_per_node: 1
      communicator: "OpenMpi"
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage1-gpt-j
        -m "EleutherAI/gpt-j-6B" -s 2048 16
    submit_args:
      container_args:
        shm_size: 512g # hai6 is dgx2
  - name: stage1_finetune_gpt-j_2xG16_nompi
    sku: 2xG16
    aml_mpirun:
      process_count_per_node: 0
      communicator: "OpenMpi"
    command:
      - >
        bash finetune-gpt-neo_cli.sh -j ft-stage1-gpt-j
        -m "EleutherAI/gpt-j-6B" -s 1536 16

# - name: csharp_dist
#   sku: G4
#   sku_count: 16
#   aml_mpirun:
#     process_count_per_node: 1
#     communicator: "OpenMpi"
#   command:
#   - bash run.sh 16
