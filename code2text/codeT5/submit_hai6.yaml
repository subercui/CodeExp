description: run on hai6

target:
  service: amlk8s
  # run "amlt target list amlk8s" to list the names of available AMLK8s targets
  # TARGET_NAME       SERVICE    CLUSTER           VC        WORKSPACE_NAME    RESOURCE_GROUP    METADATA
  # ----------------  ---------  ----------------  --------  ----------------  ----------------  --------------------------------------------------
  # itpeastusv100cl2  amlk8s     itpeastusv100cl2  resrchvc  resrchvc-eus      researchvc-eus    card: V100, gpus_per_vm: 4, ib: yes, gpu_mem: 16GB
  # itpeusp100cl      amlk8s     itpeusp100cl      resrchvc  resrchvc-eus      researchvc-eus    card: P100, gpus_per_vm: 4, ib: yes, gpu_mem: 16GB
  # itpwus2cpucl1     amlk8s     itpwus2cpucl1     gcrcpu    gcrcpu            gcrcpu-wus2       card: CPU, gpus_per_vm: 0, ib: no, gpu_mem: 128GB
  # itpseasiav100cl   amlk8s     itpseasiav100cl   resrchvc  resrchvc-sea      researchvc-sea    card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # itplabrr1cl1      amlk8s     itplabrr1cl1      resrchvc  resrchvc          researchvc        card: V100, gpus_per_vm: 8, ib: yes, gpu_mem: 32GB
  # itpwus2v100cl     amlk8s     itpwus2v100cl     resrchvc  resrchvc          researchvc        card: V100, gpus_per_vm: 8, ib: yes, gpu_mem: 32GB
  # itpeusp40cl       amlk8s     itpeusp40cl       resrchvc  resrchvc-eus      researchvc-eus    card: P40, gpus_per_vm: 4, ib: no, gpu_mem: 24GB
  # itpscusv100cl     amlk8s     itpscusv100cl     resrchvc  resrchvc-sc       researchvc-sc     card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # itplabrl1cl1      amlk8s     itplabrl1cl1      resrchvc  resrchvc          researchvc        card: P100, gpus_per_vm: 1, ib: yes, gpu_mem: 16GB
  # itpeastusv100cl   amlk8s     itpeastusv100cl   resrchvc  resrchvc-eus      researchvc-eus    card: V100, gpus_per_vm: 8, ib: no, gpu_mem: 16GB
  # name: itpeastusv100cl2
  # name: itpeusp100cl
  name: itphyperdgx2cl1
  vc: hai6
  # name: ms-shared

environment:
  # image: publicrepository/bert:pytorch_1.4_nccl_2.7.8_horovod_0.19.5_transformers_0.5.0_ib
  # image: deepspeed/deepspeed:latest-torch170-cuda110
  image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel
  registry: docker.io # any public registry can be specified here
  # image: my_azureml-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu:latest
  # registry: hai6cr.azurecr.io
  # username: hai6cr
  setup:
    - pip install -r requirements.txt

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/

data:
  # You need to run "python src/download_data.py" beforehand
  # to generate the dataset to be uploaded
  # don't forget to run with --upload-data
  local_dir: $CONFIG_DIR/data/

  # The data will be uploaded to your default storage.
  #   Check ``multi_storage.yaml'' for more flexibility.
  remote_dir: data/

# storage:
#   shared_data:
#     storage_account_name: haotian
#     container_name: home
#     mount_dir: /mnt/shared_data

env_defaults:
  LENGTH: 1024

# list of jobs to run
# avalaible sku options: ['G0', 'G1', 'G2', 'G4', 'G8', 'G16',
# '16G1', '16G4', '16G8', '24G1', '24G2', '24G4', '24G8',
# '32G4', '32G8', '32G16', '64G4', '40G8']
jobs:
  - name: stage1_finetune_codeT5
    sku: G16
    command:
      - bash finetune-codeT5_cli.sh -j ft-stage1-codeT5 -m "Salesforce/codet5-base" 16
  - name: stage1_finetune_codeT5_muti_sum
    sku: G16
    command:
      - >
        bash finetune-codeT5_cli.sh -j ft-stage1-codeT5
        -m "Salesforce/codet5-base-multi-sum" 16
  - name: stage1_finetune_codeT5_len$LENGTH
    sku: G16
    command:
      - >
        bash finetune-codeT5_cli.sh -j ft-stage1-codeT5
        -m "Salesforce/codet5-base" -s $LENGTH 16
  - name: stage2_finetune_codeT5
    sku: G16
    command:
      - >
        bash finetune-codeT5_cli.sh -j ft-stage2-codeT5
        -m "/mnt/default/ft-stage1-codeT5-GithubCodeDoc-Mar07-19-47-2022/checkpoint-420000" 16
